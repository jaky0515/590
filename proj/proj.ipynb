{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modules - include all modules here\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from pprint import pprint\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Input, Dense, Activation, Reshape, Lambda, LSTM, Dropout, Conv2D, MaxPooling2D, Embedding\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.merge import add, concatenate\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "List of classes\n",
    "\"\"\"\n",
    "\n",
    "# DataElement Class\n",
    "class DataElement:\n",
    "    \"\"\"\n",
    "    This object contains variables for a single data\n",
    "    @author: Kevin Jang (kj460)\n",
    "    \"\"\"\n",
    "    label = None        # label (tag number) of this plate\n",
    "    height = None       # height of this plate\n",
    "    width = None        # width of this plate\n",
    "    label_length = None # number of characters in this plate's label\n",
    "    img = None          # parsed image\n",
    "    tags = []           # data tags\n",
    "    \n",
    "    def __init__( self, label, height, width, label_length, img, tags ):\n",
    "        \"\"\"\n",
    "        Constructor for this class\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        self.label = label\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.label_length = label_length\n",
    "        self.img = img\n",
    "        self.tags = tags\n",
    "    \n",
    "    def __str__( self ):\n",
    "        \"\"\"\n",
    "        Returns the value of each variable for this class\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        return \"\\t** Values:\\n\\t\\t* label : {}\\n\\t\\t* height : {}\\n\\t\\t* width : {}\\n\\t\\t* label_length = {}\\n\\t\\t* img = {}\\n\\t\\t* tags = {}\\n\".format( self.label, self.height, self.width, self.label_length, self.img, self.tags )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "# DataSet Class\n",
    "class DataSet:\n",
    "    \"\"\"\n",
    "    This object contains the list of DataElement for a single dataset\n",
    "    @author: Kevin Jang (kj460)\n",
    "    \"\"\"\n",
    "    data_path = None  # path of the directory that contains data files\n",
    "    data = {}         # list of DataElement\n",
    "    num_redundant = 0 # number of redundant data\n",
    "    num_missing = 0   # number of missing data\n",
    "    num_noisy = 0     # number of noisy data\n",
    "\n",
    "    def __init__( self, data_path ):\n",
    "        \"\"\"\n",
    "        Constructor for this class\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "    \n",
    "    def __str__( self ):\n",
    "        \"\"\"\n",
    "        Returns the state of each DataElement in the data list\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        string = \"\"\n",
    "        for label, data_elem in self.data.items():\n",
    "            string += str( data_elem )\n",
    "        return string\n",
    "    \n",
    "    def valid_json( self, json_data ):\n",
    "        \"\"\"\n",
    "        Returns True if a given json_data is valid else return False\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        if 'description' not in json_data or 'size' not in json_data or 'height' not in json_data[ 'size'] or 'width' not in json_data[ 'size' ] or 'tags' not in json_data or len( json_data[ 'tags' ] ) == 0:\n",
    "            self.num_missing += 1\n",
    "            return False\n",
    "        elif len( json_data[ 'description' ] ) != 8 or int( json_data[ 'size' ][ 'width' ] ) != 152 or int( json_data[ 'size' ][ 'height' ] ) != 34:\n",
    "            self.num_noisy += 1\n",
    "            return False\n",
    "        elif json_data[ 'description' ] in self.data.keys():\n",
    "            self.num_redundant += 1\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def build_dataset( self, num_files ):\n",
    "        \"\"\"\n",
    "        Reads data files and create DataElement for each data file and include it in the data list\n",
    "        @author: Kevin Jang (kj460)\n",
    "        @params:\n",
    "            num_files - number of data files to be read; set this to 'None' to read all the files\n",
    "        \"\"\"\n",
    "        ann_path = self.data_path + '/ann'\n",
    "        img_path = self.data_path + '/img'\n",
    "        count = 0\n",
    "        for file in os.listdir( ann_path ):\n",
    "            if num_files != None and count >= num_files:\n",
    "                # stop reading\n",
    "                break\n",
    "            # file validation\n",
    "            if '.json' not in file:\n",
    "                continue\n",
    "            # process JSON file\n",
    "            json_file = open( ann_path + '/' + file )\n",
    "            json_data = json.load( json_file )\n",
    "            # data validation\n",
    "            if not self.valid_json( json_data ):\n",
    "                continue\n",
    "            # create a new DataElement\n",
    "            data_elem = DataElement( json_data[ 'description' ],\n",
    "                                   json_data[ 'size' ][ 'height' ],\n",
    "                                   json_data[ 'size' ][ 'width' ],\n",
    "                                   len( json_data[ 'description' ] ),\n",
    "                                   None,\n",
    "                                   json_data[ 'tags' ] )\n",
    "            # process PNG file\n",
    "            img_file = cv2.imread( img_path + '/' + ( file.split( '.json' )[ 0 ] ) + '.png' )\n",
    "            img_file = cv2.cvtColor( img_file, cv2.COLOR_BGR2GRAY )\n",
    "            img_file = cv2.resize( img_file, ( data_elem.width, data_elem.height ) )\n",
    "            img_file = img_file.astype( np.float32 ) / 255\n",
    "            # add a new DataElement to the list\n",
    "            data_elem.img = img_file\n",
    "            self.data[ data_elem.label ] = data_elem\n",
    "            count += 1\n",
    "            \n",
    "# TrainTestDataSet Class\n",
    "class TrainTestDataSet:\n",
    "    \"\"\"\n",
    "    This object contains DataSet for training and testing\n",
    "    @author: Kevin Jang (kj460)\n",
    "    \"\"\"\n",
    "    train_data_path = None # path of the directory that contains training data files\n",
    "    test_data_path = None  # path of the directory that contains testing data files\n",
    "    train_dataset = None   # training DataSet object\n",
    "    test_dataset = None    # testing DataSet object\n",
    "    \n",
    "    def __init__( self, train_data_path, test_data_path ):\n",
    "        \"\"\"\n",
    "        Constructor for this class\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        self.train_data_path = train_data_path\n",
    "        self.test_data_path = test_data_path\n",
    "        \n",
    "    def __str__( self ):\n",
    "        \"\"\"\n",
    "        Returns the string that contains information about training and testing dataset\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        return '*** Training Set ***\\n' + str( self.train_dataset ) + '\\n*** Testing Set ***\\n' + str( self.test_dataset )\n",
    "    \n",
    "    def build_train_test_dataset( self ):\n",
    "        \"\"\"\n",
    "        Builds training and testing DataSet\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        self.train_dataset = DataSet( self.train_data_path )\n",
    "        self.train_dataset.build_dataset( None )\n",
    "        self.test_dataset = DataSet( self.test_data_path )\n",
    "        self.test_dataset.build_dataset( None )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataValidator Class\n",
    "class DataValidator:\n",
    "    '''\n",
    "    Validator class to check the data cleanliness\n",
    "    @author: Kevin Jang (kj460)\n",
    "    '''\n",
    "    train_test_dataset = None\n",
    "    \n",
    "    def __init__( self, train_test_dataset ):\n",
    "        \"\"\"\n",
    "        Constructor for this class\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        self.train_test_dataset = train_test_dataset\n",
    "        \n",
    "    def __str__( self ):\n",
    "        \"\"\"\n",
    "        Returns the string that contains information about validation on both training and testing dataset\n",
    "        @author: Kevin Jang (kj460)\n",
    "        \"\"\"\n",
    "        str = '*** DataSet Validation ***\\n'\n",
    "        str += '\\t** Number of Redundant Data\\n'\n",
    "        str += '\\t\\t* Training DataSet : {}\\n'.format( self.train_test_dataset.train_dataset.num_redundant )\n",
    "        str += '\\t\\t* Testing DataSet : {}\\n'.format( self.train_test_dataset.test_dataset.num_redundant )\n",
    "        str += '\\t** Number of Missing Data\\n'\n",
    "        str += '\\t\\t* Training DataSet : {}\\n'.format( self.train_test_dataset.train_dataset.num_missing )\n",
    "        str += '\\t\\t* Testing DataSet : {}\\n'.format( self.train_test_dataset.test_dataset.num_missing )\n",
    "        str += '\\t** Number of Noisy Data\\n'\n",
    "        str += '\\t\\t* Training DataSet : {}\\n'.format( self.train_test_dataset.train_dataset.num_noisy )\n",
    "        str += '\\t\\t* Testing DataSet : {}\\n'.format( self.train_test_dataset.test_dataset.num_noisy )\n",
    "        return str\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directiory that contains the data files\n",
    "train_data_path = 'data/ANPR_OCR__train'\n",
    "test_data_path = 'data/ANPR_OCR__test'\n",
    "\n",
    "# create TrainTestDataSet\n",
    "train_test_dataset = TrainTestDataSet( train_data_path, test_data_path )\n",
    "train_test_dataset.build_train_test_dataset()\n",
    "\n",
    "# print the dataset\n",
    "# print( str( train_test_dataset ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** DataSet Validation ***\n",
      "\t** Number of Redundant Data\n",
      "\t\t* Training DataSet : 0\n",
      "\t\t* Testing DataSet : 0\n",
      "\t** Number of Missing Data\n",
      "\t\t* Training DataSet : 0\n",
      "\t\t* Testing DataSet : 0\n",
      "\t** Number of Noisy Data\n",
      "\t\t* Training DataSet : 0\n",
      "\t\t* Testing DataSet : 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the cleanliness of the dataset\n",
    "data_validator = DataValidator( train_test_dataset )\n",
    "print( str ( data_validator ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11382\n",
      "2256\n"
     ]
    }
   ],
   "source": [
    "training_set = {}\n",
    "random.random()\n",
    "for key, value in train_test_dataset.train_dataset.data.items():\n",
    "    rand = random.randint(1, 5)\n",
    "    if rand is 3:\n",
    "        training_set[key] = value\n",
    "print(len(train_test_dataset.train_dataset.data))\n",
    "print(len(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = set()\n",
    "for key, value in train_test_dataset.train_dataset.data.items():\n",
    "    for c in key:\n",
    "        char_set.add(c)\n",
    "\n",
    "char_list = sorted(list(char_set))\n",
    "height = 34\n",
    "width = 152\n",
    "\n",
    "def label_to_indices(label):\n",
    "    return list(map(lambda x: char_list.index(x), label))\n",
    "\n",
    "def indices_to_label(indices):\n",
    "    return ''.join(list(map(lambda x: char_list[int(x)], indices)))\n",
    "\n",
    "def decoding_to_label(y):\n",
    "    result = []\n",
    "    for i in range(y.shape[0]):\n",
    "        prev = -1\n",
    "        max_index = 0\n",
    "        s = ''\n",
    "        for j in range(2, len(y[0])):\n",
    "            for k in range(len(y[0][0])):\n",
    "                if y[i][j][k] > y[i][j][max_index]:\n",
    "                    max_index = k\n",
    "            if max_index is not prev and max_index < len(char_list):\n",
    "                s = s + char_list[max_index]\n",
    "            prev = max_index\n",
    "        result.append(s)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgLabelGenerator:\n",
    "    def __init__(self, ds, batch_size, down_factor):\n",
    "        self.label_len = 8\n",
    "        self.data_len = len(ds)\n",
    "        self.indices = list(range(self.data_len))\n",
    "        self.cur_index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.down_factor = down_factor\n",
    "        self.imgs = np.zeros((self.data_len, height, width))\n",
    "        self.labels = []\n",
    "        for i, (key, value) in enumerate(ds.items()):\n",
    "            self.labels.append(key)\n",
    "            self.imgs[i, :, :] = value.img\n",
    "        \n",
    "    def next_element(self):\n",
    "        if self.data_len-1 > self.cur_index:\n",
    "            self.cur_index = self.cur_index + 1\n",
    "        else:\n",
    "            random.shuffle(self.indices)\n",
    "            self.cur_index = 0\n",
    "        position = self.indices[self.cur_index]\n",
    "        return (self.labels[position], self.imgs[position])\n",
    "\n",
    "    def next_batch(self):\n",
    "        while True:\n",
    "            X = np.ones([self.batch_size, width, height, 1])\n",
    "            y = np.ones([self.batch_size, self.label_len])\n",
    "            input_arr = np.ones((self.batch_size, 1)) * (width // self.down_factor - 2)\n",
    "            label_arr = np.zeros((self.batch_size, 1))\n",
    "            for i in range(self.batch_size):\n",
    "                label, img = self.next_element()\n",
    "                img = np.expand_dims(img.T, -1)\n",
    "                X[i] = img\n",
    "                y[i] = label_to_indices(label)\n",
    "                label_arr[i] = len(label)\n",
    "            \n",
    "            inputs = {'input': X, 'labels': y, 'input_arr': input_arr, 'label_arr': label_arr}\n",
    "            outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "            yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 152, 34, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 152, 34, 16)  160         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_32 (AveragePo (None, 76, 17, 16)   0           conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 76, 17, 16)   2320        average_pooling2d_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_33 (AveragePo (None, 38, 8, 16)    0           conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_49 (Reshape)            (None, 38, 128)      0           average_pooling2d_33[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_91 (Dense)                (None, 38, 16)       2064        reshape_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_166 (LSTM)                 (None, 38, 128)      74240       dense_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_167 (LSTM)                 (None, 38, 128)      74240       dense_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_168 (LSTM)                 (None, 38, 128)      131584      lstm_166[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_169 (LSTM)                 (None, 38, 128)      131584      lstm_166[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_170 (LSTM)                 (None, 38, 128)      131584      lstm_167[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_171 (LSTM)                 (None, 38, 128)      131584      lstm_167[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 38, 128)      0           lstm_168[0][0]                   \n",
      "                                                                 lstm_169[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 38, 128)      0           lstm_170[0][0]                   \n",
      "                                                                 lstm_171[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 38, 256)      0           add_48[0][0]                     \n",
      "                                                                 add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 38, 23)       5911        concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sf_result (Activation)          (None, 38, 23)       0           dense_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_arr (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_arr (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           sf_result[0][0]                  \n",
      "                                                                 labels[0][0]                     \n",
      "                                                                 input_arr[0][0]                  \n",
      "                                                                 label_arr[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 685,271\n",
      "Trainable params: 685,271\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def lambda_func(args):\n",
    "    y_pred, labels, input_arr, label_arr = args\n",
    "    return K.ctc_batch_cost(labels, y_pred[:, 2:, :], input_arr, label_arr)\n",
    "\n",
    "ilg_train = ImgLabelGenerator(train_test_dataset.train_dataset.data, 8, 4)\n",
    "#ilg_train = ImgLabelGenerator(training_set, 8, 4)\n",
    "\n",
    "filter_num = 16\n",
    "\n",
    "input_data = Input(name='input', shape=(width, height, 1), dtype='float32')\n",
    "x = Conv2D(filters=filter_num, kernel_size=(3, 3), padding='same', activation='relu')(input_data)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(filters=filter_num, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "x = AveragePooling2D(pool_size=(2, 2))(x)\n",
    "x = Reshape(target_shape=(width // 4, (height // 4) * filter_num))(x)\n",
    "x = Dense(filter_num, activation='relu')(x)\n",
    "\n",
    "lstm_11 = LSTM(128, return_sequences=True, kernel_initializer='lecun_uniform')(x)\n",
    "lstm_12 = LSTM(128, return_sequences=True, go_backwards=True, kernel_initializer='lecun_uniform')(x)\n",
    "lstm_21 = LSTM(128, return_sequences=True, kernel_initializer='lecun_uniform')(lstm_11)\n",
    "lstm_22 = LSTM(128, return_sequences=True, go_backwards=True, kernel_initializer='lecun_uniform')(lstm_11)\n",
    "lstm_23 = LSTM(128, return_sequences=True, kernel_initializer='lecun_uniform')(lstm_12)\n",
    "lstm_24 = LSTM(128, return_sequences=True, go_backwards=True, kernel_initializer='lecun_uniform')(lstm_12)\n",
    "lstm21_merged = add([lstm_21, lstm_22])\n",
    "lstm22_merged = add([lstm_23, lstm_24])\n",
    "lstm3_merged = concatenate([lstm21_merged, lstm22_merged])\n",
    "x = Dense(len(char_list)+1, kernel_initializer='lecun_uniform')(lstm3_merged)\n",
    "y_pred = Activation('softmax', name='sf_result')(x)\n",
    "\n",
    "labels = Input(name='labels', shape=[ilg_train.label_len], dtype='float32')\n",
    "input_arr = Input(name='input_arr', shape=[1], dtype='int64')\n",
    "label_arr = Input(name='label_arr', shape=[1], dtype='int64')\n",
    "loss_out = Lambda(lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_arr, label_arr])\n",
    "model = Model(inputs=[input_data, labels, input_arr, label_arr], outputs=loss_out)\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "11382/11382 [==============================] - 1656s 145ms/step - loss: 1.7090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xbde97cc88>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=ilg_train.next_batch(), steps_per_epoch=ilg_train.data_len, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilg_test = ImgLabelGenerator(train_test_dataset.test_dataset.data, 256, 4)\n",
    "\n",
    "total_len = len(train_test_dataset.test_dataset.data)\n",
    "correct = 0\n",
    "count = 0\n",
    "misclassified = []\n",
    "\n",
    "for input_set, output_set in ilg_test.next_batch():\n",
    "    output_value = sess.run(model.get_layer(name='sf_result').output, feed_dict={model.get_layer(name='input').input : input_set['input']})\n",
    "    pred_labels = decoding_to_label(output_value)\n",
    "\n",
    "    for i, label in enumerate(input_set['labels']):\n",
    "        label = indices_to_label(label)\n",
    "        if label == pred_labels[i]:\n",
    "            correct = correct + 1\n",
    "        else:\n",
    "            misclassified.append([label, pred_labels[i]])\n",
    "        count = count + 1\n",
    "        if count == total_len:\n",
    "            break\n",
    "        label_list.append(label)\n",
    "    \n",
    "    if count == total_len:\n",
    "        break\n",
    "\n",
    "print(\"Correct: \" + str(correct))\n",
    "print(\"Total: \" + str(total_len))\n",
    "print(\"The accuracy rate is: \" + str(correct / total_len) + \"\\n\\n\")\n",
    "\n",
    "print(\"Misclassified License Plate:\")\n",
    "print(\"True        Predicted\")\n",
    "for mis_plate in misclassified:\n",
    "    print(mis_plate[0] + \"    \" + mis_plate[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
